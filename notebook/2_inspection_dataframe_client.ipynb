{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò Data Quality Audit ‚Äî `inspect_df`\n",
    "\n",
    "This notebook delivers a **premium, client-facing data quality report** built on your `inspect_df` helper.\n",
    "\n",
    "It is designed as a reusable asset for:\n",
    "- Data audits\n",
    "- Dataset onboarding diagnostics\n",
    "- Pre-ML data health checks\n",
    "- Client deliverables in data consulting missions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbecff8",
   "metadata": {},
   "source": [
    "## üìë Table of Contents\n",
    "\n",
    "- [Part A ‚Äî Executive & Strategic Overview](#part-a-2)\n",
    "- [Part B ‚Äî Technical Audit](#part-b-2)\n",
    "- [Code Appendix & Execution](#part-c-2)\n",
    "- [Run on cleaned animal dataset](#run-cleaned-2)\n",
    "- [Generic usage for any CSV](#generic-2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1635877",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# üß† Part A ‚Äî Executive & Strategic Overview\n",
    "<a id=\"part-a-2\"></a>\n",
    "\n",
    "### üéØ Purpose\n",
    "This notebook provides an **end-to-end structural and quality assessment** of a tabular dataset.\n",
    "\n",
    "Using a single function, `inspect_df`, it quickly answers the questions:\n",
    "- *What does this dataset look like structurally?*\n",
    "- *Which columns are present, and in which formats?*\n",
    "- *How much missingness, duplication, or inconsistency should we expect?*\n",
    "- *Is the dataset ready for downstream EDA or machine learning, or do we need cleaning first?*\n",
    "\n",
    "### üíº Value for stakeholders\n",
    "- Reduces uncertainty around data readiness.\n",
    "- Exposes data quality risks **before** investing time in modeling.\n",
    "- Creates a transparent, shareable audit trail between data teams and business teams.\n",
    "- Can be reused across projects as a standard *Data Quality Report*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîó Position in the pipeline\n",
    "\n",
    "This inspection sits **early in the data lifecycle**, typically after raw ingestion and basic structural fixes.\n",
    "\n",
    "Example pipeline:\n",
    "1. **Autofix** ‚Äî normalize CSV separator and column names (initial structural cleaning).\n",
    "2. **Inspect_df (this notebook)** ‚Äî perform a full structural and statistical scan.\n",
    "3. **Cleaning / Parsing** ‚Äî advanced normalization, mapping, feature engineering.\n",
    "4. **EDA & Modeling** ‚Äî visualizations, feature selection, ML models.\n",
    "\n",
    "Integrating `inspect_df` early avoids wasting time building models on broken data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùì Key questions this report answers\n",
    "\n",
    "- Are there **enough rows and columns** to support the planned analysis?\n",
    "- Which columns are **numeric vs categorical**, and are the types correct?\n",
    "- Where do we see **missing values**, and how severe is the problem?\n",
    "- Are there **duplicate rows** that might bias statistics or ML training?\n",
    "- Do any columns show suspicious distributions (e.g. constant values, extreme imbalance)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c4ca7b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Part B ‚Äî Technical Audit (How It Works)\n",
    "<a id=\"part-b-2\"></a>\n",
    "\n",
    "We now document the `inspect_df` utility function and how to interpret each block of output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Original function\n",
    "\n",
    "```python\n",
    "def inspect_df(df: pd.DataFrame, name: str = None, n: int = 20) -> None:\n",
    "\n",
    "    title = f\"=== DataFrame Inspection: {name} ===\" if name else \"=== DataFrame Inspection ===\"\n",
    "    print(f\"\\n==={title}===\")\n",
    "    print(\"=\" * len(title))\n",
    "\n",
    "    print(\"\\n=== Dimension ===\")\n",
    "    print(df.shape)\n",
    "\n",
    "    print(\"\\n=== DF Info ===\")\n",
    "    df.info()\n",
    "\n",
    "    print(f\"\\n=== {n} First Rows ===\")\n",
    "    display(df.head(n))\n",
    "\n",
    "    print(f\"\\n=== {n} Random Rows ===\")\n",
    "    display(df.sample(n, random_state=42))\n",
    "\n",
    "    print(\"\\n=== Descriptive Stats ===\")\n",
    "    display(df.describe(include=\"all\").T)\n",
    "\n",
    "    print(\"\\n=== Unique Value ===\")\n",
    "    print(df.nunique())\n",
    "\n",
    "    print(\"\\n=== Number of NaN Values ===\")\n",
    "    print(df.isna().sum())\n",
    "\n",
    "    print(\"\\n=== Number of Duplicates Rows ===\")\n",
    "    print(df.duplicated().sum())\n",
    "\n",
    "    print(\"\\n=== Duplicates Rows ===\")\n",
    "    print(df[df.duplicated()])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Function signature and title\n",
    "\n",
    "- `df: pd.DataFrame` ‚Üí the dataset to inspect.\n",
    "- `name: str | None` ‚Üí optional label used in the report title.\n",
    "- `n: int` ‚Üí number of rows to display for head and random sampling.\n",
    "\n",
    "The title banner:\n",
    "\n",
    "```python\n",
    "title = f\"=== DataFrame Inspection: {name} ===\" if name else \"=== DataFrame Inspection ===\"\n",
    "print(f\"\\n==={title}===\")\n",
    "print(\"=\" * len(title))\n",
    "```\n",
    "\n",
    "gives a clear entry point for readers, especially when multiple datasets are inspected in the same notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dimensions and schema\n",
    "\n",
    "```python\n",
    "print(\"\\n=== Dimension ===\")\n",
    "print(df.shape)\n",
    "\n",
    "print(\"\\n=== DF Info ===\")\n",
    "df.info()\n",
    "```\n",
    "\n",
    "- `df.shape` reports `(n_rows, n_cols)`.\n",
    "- `df.info()` exposes:\n",
    "  - column names\n",
    "  - data types (`int64`, `float64`, `object`, etc.)\n",
    "  - non-null counts\n",
    "\n",
    "**Interpretation:**\n",
    "- A sudden mismatch between expected columns and actual columns often signals ingestion issues.\n",
    "- Incorrect dtypes (e.g. numeric columns stored as `object`) indicate the need for parsing / casting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 First and random rows\n",
    "\n",
    "```python\n",
    "display(df.head(n))\n",
    "display(df.sample(n, random_state=42))\n",
    "```\n",
    "\n",
    "- `head(n)` shows the **top rows** ‚Äî typically where structural problems appear.\n",
    "- `sample(n)` shows **rows from across the dataset** with a fixed random seed.\n",
    "\n",
    "**Why both are needed:**\n",
    "- Top rows may look clean while anomalies live deeper in the file.\n",
    "- Sampling avoids a biased view based only on the first rows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Descriptive statistics, cardinality, missingness, duplicates\n",
    "\n",
    "```python\n",
    "display(df.describe(include=\"all\").T)\n",
    "\n",
    "print(df.nunique())\n",
    "print(df.isna().sum())\n",
    "print(df.duplicated().sum())\n",
    "print(df[df.duplicated()])\n",
    "```\n",
    "\n",
    "- `df.describe(include=\"all\").T` ‚Üí numeric + categorical stats in a unified table.\n",
    "- `df.nunique()` ‚Üí number of unique values per column (helps detect IDs, categories, binary flags).\n",
    "- `df.isna().sum()` ‚Üí missing values per column (critical for imputation strategy).\n",
    "- `df.duplicated()` ‚Üí detects fully duplicated rows.\n",
    "\n",
    "**Interpretation tips for a client:**\n",
    "- High missingness (>30‚Äì40%) on key features suggests the need for either domain-specific imputation or feature dropping.\n",
    "- Extremely low cardinality columns (e.g. only 1 unique value) may not add value to ML models.\n",
    "- Duplicates in transactional or event data can significantly bias metrics and forecasts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ff04c3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üíª Code Appendix & Execution\n",
    "<a id=\"part-c-2\"></a>\n",
    "Below is a self-contained implementation of `inspect_df` and example execution cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_df(df: pd.DataFrame, name: str | None = None, n: int = 20) -> None:\n",
    "    \"\"\"Comprehensive DataFrame inspection utility.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The dataset to inspect.\n",
    "    name : str | None, optional\n",
    "        Optional label displayed in the report header.\n",
    "    n : int, default=20\n",
    "        Number of rows to show in head() and sample().\n",
    "    \"\"\"\n",
    "\n",
    "    title = f\"=== DataFrame Inspection: {name} ===\" if name else \"=== DataFrame Inspection ===\"\n",
    "    print(f\"\\n==={title}===\")\n",
    "    print(\"=\" * len(title))\n",
    "\n",
    "    print(\"\\n=== Dimension ===\")\n",
    "    print(df.shape)\n",
    "\n",
    "    print(\"\\n=== DF Info ===\")\n",
    "    df.info()\n",
    "\n",
    "    print(f\"\\n=== {n} First Rows ===\")\n",
    "    display(df.head(n))\n",
    "\n",
    "    print(f\"\\n=== {n} Random Rows ===\")\n",
    "    display(df.sample(n, random_state=42))\n",
    "\n",
    "    print(\"\\n=== Descriptive Stats ===\")\n",
    "    display(df.describe(include=\"all\").T)\n",
    "\n",
    "    print(\"\\n=== Unique Value ===\")\n",
    "    print(df.nunique())\n",
    "\n",
    "    print(\"\\n=== Number of NaN Values ===\")\n",
    "    print(df.isna().sum())\n",
    "\n",
    "    print(\"\\n=== Number of Duplicates Rows ===\")\n",
    "    print(df.duplicated().sum())\n",
    "\n",
    "    print(\"\\n=== Duplicates Rows ===\")\n",
    "    print(df[df.duplicated()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ñ∂Ô∏è Run the inspection on your cleaned animal dataset\n",
    "<a id=\"run-cleaned-2\"></a>\n",
    "\n",
    "By default, we target the file produced by the `autofix` step:\n",
    "`data/raw/animal_data_dirty_reworked.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data/raw/animal_data_dirty_reworked.csv\"\n",
    "\n",
    "if os.path.exists(DATA_PATH):\n",
    "    df = pd.read_csv(DATA_PATH, sep=\";\")\n",
    "    filename = os.path.basename(DATA_PATH)\n",
    "    inspect_df(df, name=filename, n=20)\n",
    "else:\n",
    "    print(f\"‚ùå File not found: {DATA_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÅ Generic usage for any CSV\n",
    "<a id=\"generic-2\"></a>\n",
    "You can reuse this notebook across projects by changing the `DATA_PATH` below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example ‚Äî adapt this path to any dataset you want to audit\n",
    "custom_path = \"data/raw/your_other_dataset.csv\"\n",
    "\n",
    "if os.path.exists(custom_path):\n",
    "    df_custom = pd.read_csv(custom_path, sep=\";\")\n",
    "    inspect_df(df_custom, name=os.path.basename(custom_path), n=20)\n",
    "else:\n",
    "    print(f\"(Info) Custom path does not exist yet: {custom_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
