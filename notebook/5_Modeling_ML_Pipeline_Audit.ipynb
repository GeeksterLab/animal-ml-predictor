{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f2c5321",
   "metadata": {},
   "source": [
    "# ðŸ§  Modeling & ML Pipeline Audit â€” Board-Level Report\n",
    "\n",
    "**Project:** Animal Morphology & Weight Prediction  \n",
    "**Scope:** End-to-end **Machine Learning (ML) pipeline**, from feature-ready data to trained models and exported artefacts.  \n",
    "**Audience:** Câ€‘suite, product owners, data/ML leads, senior engineers.\n",
    "\n",
    "This report is designed as a **consultingâ€‘grade artifact**, suitable for both **executive review** and **technical followâ€‘up**.  \n",
    "It provides:\n",
    "\n",
    "- A **strategic view** of how the ML layer supports business objectives,  \n",
    "- A **technical audit** of the modelling pipeline and its robustness,  \n",
    "- A **roadmap** to move from â€œgood prototypeâ€ to **enterpriseâ€‘ready ML system**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e33a36",
   "metadata": {},
   "source": [
    "## ðŸ“‘ Table of Contents\n",
    "\n",
    "- [Part A â€” Executive Summary](#part-a--executive-summary)  \n",
    "- [Part B â€” Context & Modeling Objectives](#part-b--context--modeling-objectives)  \n",
    "- [Part C â€” Methodology & Evaluation Framework](#part-c--methodology--evaluation-framework)  \n",
    "- [Part D â€” Detailed ML Pipeline Audit](#part-d--detailed-ml-pipeline-audit)  \n",
    "  - [1. Data Ingestion & Feature Readiness](#1-data-ingestion--feature-readiness)  \n",
    "  - [2. Train/Validation/Test Strategy](#2-trainvalidationtest-strategy)  \n",
    "  - [3. Feature Engineering & Encoding](#3-feature-engineering--encoding)  \n",
    "  - [4. Model Choice & Hyperparameter Strategy](#4-model-choice--hyperparameter-strategy)  \n",
    "  - [5. Evaluation Metrics & Overfitting Control](#5-evaluation-metrics--overfitting-control)  \n",
    "  - [6. Model Persistence, Versioning & Registry](#6-model-persistence-versioning--registry)  \n",
    "  - [7. Inference Pipeline & Integration](#7-inference-pipeline--integration)  \n",
    "- [Architecture Stack](#marchitecture-stack)  \n",
    "- [Client Recommendation Roadmap](#client-recommendation-roadmap)  \n",
    "- [Impact Analysis](#impact-analysis)  \n",
    "- [Implementation Considerations](#implementation-considerations)  \n",
    "- [Code Appendix & Reproducible Templates](#code-appendix--reproducible-templates)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad93688f",
   "metadata": {},
   "source": [
    "## Part A â€” Executive Summary\n",
    "\n",
    "### 1. Role of the ML Pipeline in the Overall Product\n",
    "\n",
    "The **ML pipeline** is the layer that transforms cleaned and engineered features into **predictive models** capable of:\n",
    "\n",
    "- Estimating **animal weight** from observable features,  \n",
    "- Supporting **analytics and whatâ€‘if scenarios**,  \n",
    "- Powering downstream applications (notebooks, Streamlit app, APIâ€‘like interfaces).\n",
    "\n",
    "Its quality directly affects:\n",
    "\n",
    "- Prediction reliability,  \n",
    "- Business trust in AIâ€‘driven recommendations,  \n",
    "- The speed at which new models can be tested and deployed.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Headline Assessment\n",
    "\n",
    "The current ML pipeline represents a **solid, wellâ€‘structured prototype**:\n",
    "\n",
    "- Models are trained on a **clean dataset** with clear features,  \n",
    "- Core scikitâ€‘learn practices are followed (fit/predict, train/test split, etc.),  \n",
    "- Results are reproducible on a singleâ€‘machine / notebook basis,  \n",
    "- Artefacts (models, plots, metrics) are saved in dedicated folders under `results/`.\n",
    "\n",
    "To reach **enterpriseâ€‘grade maturity**, several aspects need to evolve:\n",
    "\n",
    "1. **Formalisation of data and model contracts**  \n",
    "   - No explicit contract for feature schema at training vs inference time.  \n",
    "   - Limited protection against **data drift** or schema changes.\n",
    "\n",
    "2. **Standardisation of training & evaluation flows**  \n",
    "   - Experimental logic and production logic are partially intertwined.  \n",
    "   - Crossâ€‘validation, hyperparameter search and evaluation are not yet fully packaged.\n",
    "\n",
    "3. **Model governance (versioning, registry, traceability)**  \n",
    "   - Trained models are saved, but without a **central registry** or rich metadata.  \n",
    "   - Harder to answer: â€œWhich model is currently in use, and why?â€\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Key Strengths\n",
    "\n",
    "- âœ… Clear separation between **data preparation** and **model training** (conceptually aligned).  \n",
    "- âœ… Use of **standard ML libraries** (scikitâ€‘learn) with predictable behaviour.  \n",
    "- âœ… Directory structure in `results/` logically groups plots, stats and models.  \n",
    "- âœ… Strong foundation for turning this into a **productionâ€‘ready ML asset**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Key Risks\n",
    "\n",
    "- âš ï¸ Risk of **silent interface mismatch** between training and inference (e.g. feature order, missing columns).  \n",
    "- âš ï¸ No systematic **experiment tracking**, making it difficult to compare runs over time.  \n",
    "- âš ï¸ Limited transparency for leadership on **model robustness and limitations**.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Top 5 Strategic Recommendations\n",
    "\n",
    "1. Define formal **feature contracts** and enforce them at training and inference.  \n",
    "2. Standardise train/validation/test strategy with **crossâ€‘validation** where relevant.  \n",
    "3. Introduce basic **experiment tracking** (metrics, params, model versions) â€“ even via CSV/JSON at first.  \n",
    "4. Establish a lightweight **model registry** to track which model is â€œcurrentâ€, â€œcandidateâ€, â€œdeprecatedâ€.  \n",
    "5. Package inference into a **single, wellâ€‘defined pipeline** function usable by the Streamlit app or other clients.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8192575",
   "metadata": {},
   "source": [
    "## Part B â€” Context & Modeling Objectives\n",
    "\n",
    "### 1. Business Context\n",
    "\n",
    "The business problem is to **predict animal weight** (a continuous target) using morphological and contextual features.  \n",
    "This supports:\n",
    "\n",
    "- Better understanding of **speciesâ€‘level patterns**,  \n",
    "- Potential operational use cases (e.g. planning, resource allocation, habitat management),  \n",
    "- Downstream analytics and visualisations for stakeholders.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Role of the ML Pipeline in the Stack\n",
    "\n",
    "The ML pipeline sits on top of **cleaned data and EDA learnings** (documented in previous audit notebooks).  \n",
    "It:\n",
    "\n",
    "- Consumes cleaned features from `data/cleaned/`,  \n",
    "- Produces **trained models** and **evaluation metrics** into `results/model/ML/`,  \n",
    "- Exposes predictions to notebooks / app layers (directly or via saved artefacts).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Modeling Objectives\n",
    "\n",
    "From an ML perspective, the objectives are to:\n",
    "\n",
    "- Build models that achieve **stable predictive performance**,  \n",
    "- Avoid **overfitting**, ensuring generalisation to new observations,  \n",
    "- Provide interpretable insights where possible (feature importance, partial effects),  \n",
    "- Be **maintainable and extensible** as data and requirements evolve.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84299ca7",
   "metadata": {},
   "source": [
    "## Part C â€” Methodology & Evaluation Framework\n",
    "\n",
    "### 1. Audit Approach\n",
    "\n",
    "We reviewed the ML pipeline along three key aspects:\n",
    "\n",
    "1. **Data & Feature Readiness**  \n",
    "   - Are inputs to the model robust, wellâ€‘defined, and consistent with EDA findings?  \n",
    "\n",
    "2. **Model Training & Evaluation**  \n",
    "   - Is the training process statistically sound and reproducible?  \n",
    "   - Are evaluation metrics appropriate and correctly interpreted?  \n",
    "\n",
    "3. **MLOps & Governance Foundations**  \n",
    "   - Are models and metrics stored in a way that supports future reuse?  \n",
    "   - Is there a clear path towards deployment and lifecycle management?  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Evaluation Dimensions\n",
    "\n",
    "| Dimension                         | Description |\n",
    "|-----------------------------------|-------------|\n",
    "| **Feature Schema & Contracts**    | Clarity and enforcement of input structure |\n",
    "| **Data Splitting Strategy**       | Train/val/test logic, avoidance of leakage |\n",
    "| **Feature Engineering & Encoding**| Handling of categorical/numeric/derived features |\n",
    "| **Model Architecture & Choice**   | Justification and appropriateness of algorithms |\n",
    "| **Evaluation Methodology**        | Metrics, validation strategy, robustness checks |\n",
    "| **Model Registry & Versioning**   | Storage, naming, and lifecycle of trained models |\n",
    "| **Inference Path**                | How predictions are generated and consumed |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Maturity Scale\n",
    "\n",
    "Using the same 1â€“5 scale as previous audits:\n",
    "\n",
    "| Level | Label         | Description |\n",
    "|-------|---------------|-------------|\n",
    "| 1     | Adâ€‘hoc        | Manual experiments, undocumented |\n",
    "| 2     | Emerging      | Some structure, limited reuse |\n",
    "| 3     | Structured    | Clear scripts, partially standardised |\n",
    "| 4     | Managed       | Formal pipelines, traceability, monitoring |\n",
    "| 5     | Optimised     | Automated, continuously improved, tightly integrated with product |\n",
    "\n",
    "The current ML pipeline is between **Level 2 and 3**: a **strong prototype** with clear opportunity to move towards **Level 4** with targeted improvements.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4c9338",
   "metadata": {},
   "source": [
    "## Part D â€” Detailed ML Pipeline Audit\n",
    "\n",
    "### 1. Data Ingestion & Feature Readiness\n",
    "\n",
    "**What we examine**\n",
    "\n",
    "- How the model training code accesses the dataset (paths, loaders, configs),  \n",
    "- Whether features used at training time are clearly defined and documented,  \n",
    "- Alignment with the cleaned dataset structure.\n",
    "\n",
    "**Findings**\n",
    "\n",
    "- âœ… Training scripts rely on **cleaned data** (e.g. `data/cleaned/clean_animal_data.csv`), which is good practice.  \n",
    "- âœ… Use of centralised path utilities (e.g. `config_utils`, `paths_utils`) reduces hardâ€‘coding.  \n",
    "- âš ï¸ Feature lists / selections may be defined **inside notebooks or scripts**, making contracts implicit.  \n",
    "\n",
    "**Risks**\n",
    "\n",
    "- If columns are added, renamed, or removed upstream, model training may break or silently behave differently.  \n",
    "\n",
    "**Recommendations**\n",
    "\n",
    "- Define an explicit **`FEATURE_SET`** (Python list or config) referenced in all training and inference code.  \n",
    "- Document which features are:  \n",
    "  - mandatory,  \n",
    "  - optional,  \n",
    "  - derived.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Train/Validation/Test Strategy\n",
    "\n",
    "**What we examine**\n",
    "\n",
    "- How data is split between training, validation, and test sets,  \n",
    "- Avoidance of target leakage,  \n",
    "- Whether the evaluation set is reused across experiments.\n",
    "\n",
    "**Findings**\n",
    "\n",
    "- âœ… Use of **train/test split** or similar scikitâ€‘learn utilities is in place.  \n",
    "- âš ï¸ The validation strategy may rely on a **single split**, which can be noisy for small/medium datasets.  \n",
    "- âš ï¸ No clear distinction between **model selection** (validation) and **final performance estimation** (test).  \n",
    "\n",
    "**Recommendations**\n",
    "\n",
    "- Adopt **kâ€‘fold crossâ€‘validation** for model selection where appropriate.  \n",
    "- Reserve a **holdâ€‘out test set** that is used *only* for final reporting, not for tuning.  \n",
    "- Log the random seed, split ratios, and number of folds in a small metadata structure.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Feature Engineering & Encoding\n",
    "\n",
    "**What we examine**\n",
    "\n",
    "- Handling of categorical variables, scaling of numerical features, creation of derived features,  \n",
    "- Whether the same transformations are applied consistently at training and inference.\n",
    "\n",
    "**Findings**\n",
    "\n",
    "- âœ… Feature engineering is relatively light and not overâ€‘complicated, which is appropriate.  \n",
    "- âœ… The project is aligned with scikitâ€‘learn pipelines, or easily can be.  \n",
    "- âš ï¸ If manual transformations are performed in notebooks, there is a risk of **train/inference mismatch**.  \n",
    "\n",
    "**Recommendations**\n",
    "\n",
    "- Encapsulate all transformations in a **scikitâ€‘learn `Pipeline`** (or compatible pattern), including:  \n",
    "  - imputers,  \n",
    "  - scalers,  \n",
    "  - encoders,  \n",
    "  - feature generators.  \n",
    "- Ensure the **same pipeline object** is saved and reused at inference time (not reimplemented adâ€‘hoc).  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. Model Choice & Hyperparameter Strategy\n",
    "\n",
    "**What we examine**\n",
    "\n",
    "- Algorithms used (e.g. Linear Regression, DecisionTreeRegressor, RandomForestRegressor, GradientBoosting, etc.),  \n",
    "- Hyperparameter configuration and tuning,  \n",
    "- Justification relative to data size and complexity.\n",
    "\n",
    "**Findings**\n",
    "\n",
    "- âœ… Models chosen are **appropriate** for tabular regression (treeâ€‘based ensembles + linear baselines).  \n",
    "- âœ… There is a natural progression from **simple baselines** to more complex models.  \n",
    "- âš ï¸ Hyperparameters may be defined **manually**, with limited structured search.  \n",
    "- âš ï¸ No lightweight framework (even grid search) is systematically embedded for key models.\n",
    "\n",
    "**Recommendations**\n",
    "\n",
    "- For each major model family, define a **default hyperparameter grid** and use scikitâ€‘learnâ€™s `GridSearchCV` or `RandomizedSearchCV`.  \n",
    "- Store **best params** and corresponding scores in a structured metrics file (JSON/CSV).  \n",
    "- Maintain a **model comparison table** (baseline vs tree models, etc.).  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. Evaluation Metrics & Overfitting Control\n",
    "\n",
    "**What we examine**\n",
    "\n",
    "- Metrics used (RMSE, MAE, RÂ², etc.),  \n",
    "- Presence of training vs validation/test comparisons,  \n",
    "- Checks for overfitting and underfitting.\n",
    "\n",
    "**Findings**\n",
    "\n",
    "- âœ… Core regression metrics (e.g. RMSE or MAE) are used.  \n",
    "- âš ï¸ The distinction between training and test metrics may not be clearly highlighted.  \n",
    "- âš ï¸ No clear **error analysis** per segment (e.g. by species), which can hide systematic biases.  \n",
    "\n",
    "**Recommendations**\n",
    "\n",
    "- Always report, side by side:  \n",
    "  - training metric,  \n",
    "  - validation/test metric,  \n",
    "  - absolute and relative difference.  \n",
    "- For treeâ€‘based models, consider a simple **errorâ€‘byâ€‘segment** view (e.g. MAE per species).  \n",
    "- Introduce basic overfitting flags, e.g.:  \n",
    "  > â€œIf train_RMSE < 0.5Ã— test_RMSE â†’ potential overfitting.â€  \n",
    "\n",
    "---\n",
    "\n",
    "### 6. Model Persistence, Versioning & Registry\n",
    "\n",
    "**What we examine**\n",
    "\n",
    "- How trained models are saved, named, and organised under `results/`,  \n",
    "- Whether metadata (date, data version, metrics) is stored with models.\n",
    "\n",
    "**Findings**\n",
    "\n",
    "- âœ… Models are persisted to disk (e.g. via `pickle`) in dedicated folders.  \n",
    "- âœ… Directory structure is logical (`results/model/ML/...`).  \n",
    "- âš ï¸ No **central index** listing:  \n",
    "  - which models exist,  \n",
    "  - which dataset version they used,  \n",
    "  - which metrics they achieved.  \n",
    "- âš ï¸ No notion of **â€œcurrent production modelâ€** vs â€œexperimental runsâ€.  \n",
    "\n",
    "**Recommendations**\n",
    "\n",
    "- Create a basic **model registry file** (CSV or JSON) with:  \n",
    "  - model_id, path, algorithm, hyperparameters, training dataset path, metrics, training date, status (`candidate`, `current`, `deprecated`).  \n",
    "- Update this registry automatically at the end of each training run.  \n",
    "- Clearly mark which model is used by the Streamlit app (or other consumers).  \n",
    "\n",
    "---\n",
    "\n",
    "### 7. Inference Pipeline & Integration\n",
    "\n",
    "**What we examine**\n",
    "\n",
    "- How predictions are generated for new data,  \n",
    "- Whether inference replicates the same transformations as training,  \n",
    "- How the Streamlit app (or other engines) call the model.\n",
    "\n",
    "**Findings**\n",
    "\n",
    "- âœ… Inference logic is feasible from existing code and saved models.  \n",
    "- âš ï¸ Inference steps might be scattered or reconstructed manually in app scripts.  \n",
    "- âš ï¸ No single **â€œpredict_one / predict_batchâ€** entry point that fully encapsulates the process.  \n",
    "\n",
    "**Recommendations**\n",
    "\n",
    "- Define a central `predict_weight(df_input)` function that:  \n",
    "  1. validates input schema,  \n",
    "  2. applies the saved preprocessing pipeline,  \n",
    "  3. loads the current best model from the registry,  \n",
    "  4. returns predictions along with relevant metadata (e.g. model_id).  \n",
    "\n",
    "This is key to enabling **stable, maintainable integration** with the Streamlit app and future APIs.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c520a05c",
   "metadata": {},
   "source": [
    "## Architecture Stack\n",
    "\n",
    "Audience: Executives, non-technical reviewers\n",
    "Objective: Convey how raw data becomes strategic insights\n",
    "\n",
    "### 1. Current ML Pipeline\n",
    "\n",
    "**Audience**: ML engineers, advanced analysts  \n",
    "**Objective**: Describe how the clean dataset is currently transformed into models and metrics, and how artefacts are exposed to consumers  \n",
    "\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "\n",
    "Clean[\"Clean Dataset (`data/cleaned/clean_animal_data.csv`)\"]\n",
    "  --> FE[\"Feature Preparation (selection, manual transforms)\"]\n",
    "FE --> Train[\"Model Training Script / Notebook\"]\n",
    "Train --> Eval[\"Metrics & Plots\"]\n",
    "Train --> ModelFile[\"Saved Model File (`results/model/ML/...`)\"]\n",
    "\n",
    "ModelFile --> App[\"Streamlit App / Notebooks\"]\n",
    "Eval --> Reports[\"Analytical Reports\"]\n",
    "```\n",
    "\n",
    "This represents a **well-organised prototype**: data flows one way, artefacts are saved, but **schema expectations, registry, and governance remain implicit**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Target-State ML Architecture â€” Configurable, Governed Pipeline\n",
    "\n",
    "**Audience**: Tech leads, MLOps engineers, architecture reviewers   \n",
    "**Objective**: Make explicit the separation between data, training, registry and inference, with clear traceability   \n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "\n",
    "subgraph Data[\"Data Layer\"]\n",
    "    Clean[\"Clean Feature Table\"]\n",
    "    Meta[\"Data Version & Schema Registry\"]\n",
    "end\n",
    "\n",
    "subgraph TrainBlock[\"Training Pipeline\"]\n",
    "    Conf[\"Training Config (features, params, split, metrics)\"]\n",
    "    FE[\"Preprocessing & Feature Pipeline\"]\n",
    "    CV[\"Crossâ€‘Validation & Model Selection\"]\n",
    "    Best[\"Best Model Artifact\"]\n",
    "    MMeta[\"Model Metadata (metrics, params, data version)\"]\n",
    "end\n",
    "\n",
    "subgraph Registry[\"Model Registry\"]\n",
    "    Reg[\"Model Index (JSON/CSV/DB)\"]\n",
    "end\n",
    "\n",
    "subgraph Inference[\"Inference Layer\"]\n",
    "    Inp[\"Incoming Data (App / Batch)\"]\n",
    "    Prep[\"Apply Preprocessing Pipeline\"]\n",
    "    UseModel[\"Load Current Production Model\"]\n",
    "    Pred[\"Predictions + Confidence / Metadata\"]\n",
    "end\n",
    "\n",
    "Clean --> FE\n",
    "Meta --> FE\n",
    "FE --> CV --> Best --> MMeta\n",
    "MMeta --> Reg\n",
    "Best --> Reg\n",
    "\n",
    "Inp --> Prep --> UseModel --> Pred\n",
    "UseModel --> Reg\n",
    "```\n",
    "\n",
    "In this target model, **training, registry and inference are fully aligned**: the same preprocessing pipeline is reused, the â€œcurrentâ€ model is explicitly defined, and each prediction can be traced back to data and config versions.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Executive Narrative View â€” ML Production Line\n",
    "\n",
    "**Audience**: Executives, PMs  \n",
    "**Objective**: Provide a clean overview of how ML assets are created and consumed. \n",
    "\n",
    "```ascii\n",
    "                      â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "                      â•‘        DATA LAYER        â•‘\n",
    "                      â•‘  â€¢ Clean features        â•‘\n",
    "                      â•‘  â€¢ Version registry      â•‘\n",
    "                      â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "                                   â”‚\n",
    "                                   â–¼\n",
    "                    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "                    â•‘       TRAINING LAYER       â•‘\n",
    "                    â•‘  â€¢ Config-driven training  â•‘\n",
    "                    â•‘  â€¢ Feature pipelines       â•‘\n",
    "                    â•‘  â€¢ Model selection         â•‘\n",
    "                    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "                                   â”‚\n",
    "                                   â–¼\n",
    "                      â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "                      â•‘      MODEL REGISTRY      â•‘\n",
    "                      â•‘  â€¢ Metrics + metadata    â•‘\n",
    "                      â•‘  â€¢ Current best model    â•‘\n",
    "                      â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "                                   â”‚\n",
    "                                   â–¼\n",
    "                    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "                    â•‘      INFERENCE LAYER       â•‘\n",
    "                    â•‘  â€¢ Preprocessing reuse     â•‘\n",
    "                    â•‘  â€¢ Stable predictions      â•‘\n",
    "                    â•‘  â€¢ App / batch consumers   â•‘\n",
    "                    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "```\n",
    "This schematic highlights the separation between data assets, training logic, model governance, and inference consumption. It helps stakeholders understand how models move from experimentation to controlled, production-aligned usage.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facd2a6a",
   "metadata": {},
   "source": [
    "## Client Recommendation Roadmap\n",
    "\n",
    "### 1. Shortâ€‘Term (0â€“4 weeks) â€” â€œStabilise & Make Explicitâ€\n",
    "\n",
    "- Define and centralise **feature lists** and input schemas.  \n",
    "- Refactor training scripts to clearly separate:  \n",
    "  - data loading,  \n",
    "  - preprocessing,  \n",
    "  - modelling,  \n",
    "  - evaluation,  \n",
    "  - saving artefacts.  \n",
    "- Start writing a minimal **model metadata file** (per run) capturing metrics and params.\n",
    "\n",
    "### 2. Midâ€‘Term (1â€“3 months) â€” â€œStandardise & Governâ€\n",
    "\n",
    "- Implement standard **crossâ€‘validation** and hyperparameter search for key algorithms.  \n",
    "- Build a **lightweight model registry** (CSV/JSON) updated automatically.  \n",
    "- Package a **`predict()` function** that can be used consistently across notebooks and the Streamlit app.\n",
    "\n",
    "### 3. Longâ€‘Term (3â€“6+ months) â€” â€œProductise & Monitorâ€\n",
    "\n",
    "- Integrate the ML pipeline into **CI/CD** (e.g. GitHub Actions) for automated training on new data.  \n",
    "- Extend the registry with **deployment status**, **rollâ€‘back capacity**, and basic drift checks.  \n",
    "- Consider introducing an experiment tracking tool once the volume of experiments increases.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326ff6db",
   "metadata": {},
   "source": [
    "## Impact Analysis\n",
    "\n",
    "### 1. Qualitative Impact\n",
    "\n",
    "By implementing the recommended actions, the ML pipeline will:\n",
    "\n",
    "- Move from **prototypeâ€‘grade** to **managed, productionâ€‘oriented** maturity,  \n",
    "- Provide **traceable, reproducible** model training,  \n",
    "- Make it easier to **explain and justify** model choices to both technical and nonâ€‘technical stakeholders,  \n",
    "- Reduce operational risk when integrating with applications (e.g. Streamlit app).  \n",
    "\n",
    "### 2. Quantitative Impact (Indicative)\n",
    "\n",
    "| Dimension               | Current State                           | Target State                            | Expected Impact                                      |\n",
    "|-------------------------|------------------------------------------|-----------------------------------------|------------------------------------------------------|\n",
    "| Reproducibility         | Medium (scripted, but partially implicit) | High (configs + metadata + registry)  | 40â€“60% faster debug & model reâ€‘training             |\n",
    "| Governance & Traceability | Low/Medium                             | High                                    | Clear auditability of which model runs where        |\n",
    "| Integration Stability   | Medium                                   | High                                    | Fewer runtime issues when calling models            |\n",
    "| Experiment Efficiency   | Medium                                   | High                                    | Easier comparison of models & hyperparameters       |\n",
    "\n",
    "These are directional estimates based on typical gains observed in ML projects of similar scope and complexity.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c880753b",
   "metadata": {},
   "source": [
    "## Implementation Considerations\n",
    "\n",
    "### 1. Roles & Ownership\n",
    "\n",
    "- A **Model Owner** (often an ML Engineer or advanced Data Scientist) should be appointed.  \n",
    "- Collaboration with the **Data Pipeline Owner** is essential to ensure that feature schemas stay aligned.\n",
    "\n",
    "### 2. Phasing & Risk Management\n",
    "\n",
    "- Implement changes **incrementally**, starting with:  \n",
    "  - feature contracts,  \n",
    "  - registry structure,  \n",
    "  - inference function.  \n",
    "- Avoid blocking current work: keep the prototype path available while building the more structured pipeline in parallel.\n",
    "\n",
    "### 3. Documentation\n",
    "\n",
    "- Maintain a short **â€œML Pipeline READMEâ€** summarising:  \n",
    "  - main scripts / entry points,  \n",
    "  - how to retrain a model,  \n",
    "  - where models are stored,  \n",
    "  - how to update the Streamlit app to use a new model.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3221431d",
   "metadata": {},
   "source": [
    "## Code Appendix & Reproducible Templates\n",
    "\n",
    "> â„¹ï¸ The following examples illustrate how a **minimal but robust training + inference pattern** could be structured.  \n",
    "> They are generic templates and must be adapted to your actual modules, features, and paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ae0eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Example paths (adapt to your config_utils)\n",
    "DATA_CLEAN = Path(\"data/cleaned/clean_animal_data.csv\")\n",
    "MODEL_DIR = Path(\"results/model/ML\")\n",
    "REGISTRY_PATH = MODEL_DIR / \"model_registry.json\"\n",
    "\n",
    "\n",
    "FEATURES: List[str] = [\n",
    "    # Example placeholders â€” adapt to your real feature names\n",
    "    \"Length_cm\",\n",
    "    \"Height_cm\",\n",
    "    \"Some_numeric_feature\",\n",
    "]\n",
    "\n",
    "TARGET = \"Weight_kg\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelMetadata:\n",
    "    model_id: str\n",
    "    algorithm: str\n",
    "    params: Dict[str, Any]\n",
    "    train_mae: float\n",
    "    test_mae: float\n",
    "    train_rmse: float\n",
    "    test_rmse: float\n",
    "    data_path: str\n",
    "\n",
    "\n",
    "def load_data(path: Path = DATA_CLEAN) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"âœ… Loaded data â€” shape={df.shape}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def train_random_forest(df: pd.DataFrame) -> ModelMetadata:\n",
    "    X = df[FEATURES]\n",
    "    y = df[TARGET]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=200,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "    test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "\n",
    "    MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    model_id = \"rf_v1\"  # In practice, generate unique ID per run\n",
    "    model_path = MODEL_DIR / f\"{model_id}.pkl\"\n",
    "\n",
    "    import pickle\n",
    "    with open(model_path, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    meta = ModelMetadata(\n",
    "        model_id=model_id,\n",
    "        algorithm=\"RandomForestRegressor\",\n",
    "        params=model.get_params(),\n",
    "        train_mae=train_mae,\n",
    "        test_mae=test_mae,\n",
    "        train_rmse=train_rmse,\n",
    "        test_rmse=test_rmse,\n",
    "        data_path=str(DATA_CLEAN),\n",
    "    )\n",
    "\n",
    "    _update_registry(meta)\n",
    "    print(f\"ðŸ’¾ Saved model to {model_path}\")\n",
    "    print(f\"ðŸ“Š MAE train={train_mae:.3f} â€” test={test_mae:.3f}\")\n",
    "    return meta\n",
    "\n",
    "\n",
    "def _update_registry(meta: ModelMetadata) -> None:\n",
    "    registry = []\n",
    "    if REGISTRY_PATH.exists():\n",
    "        with open(REGISTRY_PATH, \"r\") as f:\n",
    "            try:\n",
    "                registry = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                registry = []\n",
    "\n",
    "    registry.append(asdict(meta))\n",
    "    with open(REGISTRY_PATH, \"w\") as f:\n",
    "        json.dump(registry, f, indent=2)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_data()\n",
    "    metadata = train_random_forest(df)\n",
    "    print(\"âœ… Training complete; registry updated.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
